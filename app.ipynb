{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae648a1",
   "metadata": {},
   "source": [
    "Step 1: Data Loading and Initial Inspection ðŸ“Š\n",
    "First, we'll load the dataset and get a quick overview, as demonstrated previously. This ensures the data is correctly loaded and helps us understand its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f1963f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   Product_ID                         Title     Category       Brand  \\\n",
      "0        1001    Wireless Bluetooth Earbuds  Electronics   SoundBeat   \n",
      "1        1002           Men's Running Shoes      Fashion  SprintFlex   \n",
      "2        1003          Smart LED TV 43 inch  Electronics    VivoView   \n",
      "3        1004        Organic Green Tea Pack      Grocery  NatureLeaf   \n",
      "4        1005  Portable Power Bank 10000mAh  Electronics   ChargePro   \n",
      "\n",
      "                                        Key_Features  \\\n",
      "0  Bluetooth 5.3; Noise Cancellation; IPX5 Water ...   \n",
      "1       Breathable Mesh; Lightweight; Anti-slip Sole   \n",
      "2                   4K Ultra HD; Smart Apps; HDMI x3   \n",
      "3            100% Organic; Antioxidant Rich; 25 Bags   \n",
      "4              Fast Charging; Dual USB; Compact Size   \n",
      "\n",
      "                                               Specs  \\\n",
      "0              Battery: 24h; Weight: 50g; Range: 10m   \n",
      "1             Size: 7-12; Weight: 250g; Color: Black   \n",
      "2  Screen: 43in; Resolution: 3840x2160; Ports: HD...   \n",
      "3                        Net Weight: 50g; Pack of: 1   \n",
      "4     Capacity: 10000mAh; Weight: 180g; Output: 2.4A   \n",
      "\n",
      "                                         Description  \n",
      "0  Enjoy crystal-clear sound with Bluetooth 5.3 t...  \n",
      "1  Designed for performance and comfort, these ru...  \n",
      "2  Immerse yourself in stunning 4K visuals with s...  \n",
      "3  Enjoy a refreshing and healthy cup of green te...  \n",
      "4  Stay charged on-the-go with this compact and p...  \n",
      "\n",
      "Concise summary of the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Product_ID    10 non-null     int64 \n",
      " 1   Title         10 non-null     object\n",
      " 2   Category      10 non-null     object\n",
      " 3   Brand         10 non-null     object\n",
      " 4   Key_Features  10 non-null     object\n",
      " 5   Specs         10 non-null     object\n",
      " 6   Description   10 non-null     object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 692.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:/Users/ayesh/Downloads/ai_product_descriptions_dataset.csv')\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display concise summary\n",
    "print(\"\\nConcise summary of the dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa55e1",
   "metadata": {},
   "source": [
    "Step 2: Text Preprocessing ðŸ§¹\n",
    "Text data often needs cleaning before analysis. This involves converting text to lowercase, removing punctuation, and potentially removing common \"stop words\" (like \"the\", \"is\", \"a\") that don't add much meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4666d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptions after preprocessing (first 5):\n",
      "Product 1001: enjoy crystalclear sound bluetooth 53 technology ipx5 water resistance active lifestyles perfect workouts travel\n",
      "Product 1002: designed performance comfort running shoes feature breathable mesh upper antislip sole stability terrains\n",
      "Product 1003: immerse stunning 4k visuals smart features give access popular streaming apps\n",
      "Product 1004: enjoy refreshing healthy cup green tea packed natural antioxidants pack contains 25 tea bags premium quality leaves\n",
      "Product 1005: stay charged onthego compact powerful 10000mah power bank supports fast charging multiple devices simultaneously\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stop words if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = text.split() # Split into words\n",
    "    words = [word for word in words if word not in stop_words] # Remove stop words\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the 'Description' column\n",
    "df['Processed_Description'] = df['Description'].apply(preprocess_text)\n",
    "\n",
    "print(\"\\nDescriptions after preprocessing (first 5):\")\n",
    "for i, desc in enumerate(df['Processed_Description'].head()):\n",
    "    print(f\"Product {df['Product_ID'][i]}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad64e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\ayesh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\ayesh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ayesh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ayesh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ayesh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\ayesh\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654d80a",
   "metadata": {},
   "source": [
    "Step 3: Feature Extraction (Text Vectorization) âž¡ï¸ðŸ”¢\n",
    "To analyze text quantitatively, we need to convert it into numerical representations. A common method is TF-IDF (Term Frequency-Inverse Document Frequency), which reflects how important a word is to a document in a collection or corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281371c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF matrix shape: (10, 100)\n",
      "Top 10 most frequent words (features):\n",
      "['10000mah' '15l' '25' '4k' '53' 'access' 'active' 'antioxidants'\n",
      " 'antislip' 'apps']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100) # Limiting to top 100 features for simplicity\n",
    "\n",
    "# Fit and transform the processed descriptions\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Processed_Description'])\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"\\nTF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "print(\"Top 10 most frequent words (features):\")\n",
    "print(feature_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a92767b",
   "metadata": {},
   "source": [
    "Step 4: Simple Analysis: Most Frequent Words ðŸ“ˆ\n",
    "Now that we have numerical representations, we can perform simple analyses, such as finding the most frequent words in the entire corpus. This can give insights into common themes across product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7316be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most important words across all descriptions (by TF-IDF score):\n",
      "           word     score\n",
      "98        water  0.630335\n",
      "78  performance  0.598423\n",
      "69        mouse  0.585043\n",
      "97          use  0.548852\n",
      "44     features  0.532390\n",
      "99     workouts  0.532254\n",
      "96       travel  0.515955\n",
      "16         boil  0.481184\n",
      "95          tea  0.462217\n",
      "38        enjoy  0.456088\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum the TF-IDF scores for each word across all documents\n",
    "word_scores = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "# Create a DataFrame of words and their TF-IDF scores\n",
    "word_df = pd.DataFrame({'word': feature_names, 'score': word_scores})\n",
    "\n",
    "# Sort by score in descending order\n",
    "word_df = word_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important words across all descriptions (by TF-IDF score):\")\n",
    "print(word_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b711b",
   "metadata": {},
   "source": [
    "Step 5: Generating \"Optimized\" Descriptions (Hypothesis Formulation) ðŸ’¡\n",
    "For A/B testing, you need a control (original description) and a variant (optimized description). In a real scenario, this optimization would come from insights (e.g., adding keywords, making it more concise, highlighting benefits). Here, we'll simulate a simple optimization: making descriptions slightly more \"benefit-oriented\" or \"action-oriented\" by appending a phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19778f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs. Variant Descriptions (first 2 examples):\n",
      "Product ID: 1001\n",
      "Original: Enjoy crystal-clear sound with Bluetooth 5.3 technology and IPX5 water resistance for active lifestyles. Perfect for workouts and travel.\n",
      "Variant : Enjoy crystal-clear sound with Bluetooth 5.3 technology and IPX5 water resistance for active lifestyles. Perfect for workouts and travel. Experience the difference today!\n",
      "\n",
      "Product ID: 1002\n",
      "Original: Designed for performance and comfort, these running shoes feature a breathable mesh upper and anti-slip sole for stability on all terrains.\n",
      "Variant : Designed for performance and comfort, these running shoes feature a breathable mesh upper and anti-slip sole for stability on all terrains. Experience the difference today!\n"
     ]
    }
   ],
   "source": [
    "# Create a 'Variant_Description' column by simulating an \"optimization\"\n",
    "# This is a placeholder for a real optimization strategy (e.g., using NLP models to generate better text)\n",
    "df['Variant_Description'] = df['Description'].apply(lambda x: x + \" Experience the difference today!\")\n",
    "\n",
    "print(\"\\nOriginal vs. Variant Descriptions (first 2 examples):\")\n",
    "print(\"Product ID:\", df['Product_ID'][0])\n",
    "print(\"Original:\", df['Description'][0])\n",
    "print(\"Variant :\", df['Variant_Description'][0])\n",
    "print(\"\\nProduct ID:\", df['Product_ID'][1])\n",
    "print(\"Original:\", df['Description'][1])\n",
    "print(\"Variant :\", df['Variant_Description'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9bc5c4",
   "metadata": {},
   "source": [
    "Step 6: A/B Test Setup (Missing Code from Above) ðŸ§ª\n",
    "This is where we define the experiment. We'll assign users (or product views) to either the Control Group (sees original description) or the Variant Group (sees optimized description). We'll simulate user assignment and a conversion metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eca895d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulated A/B Test Data (first 10 rows):\n",
      "   user_id    group  product_id_viewed  conversion\n",
      "0      523  control               1004           0\n",
      "1      115  control               1010           0\n",
      "2       94  control               1010           0\n",
      "3      649  control               1007           0\n",
      "4      511  control               1009           0\n",
      "5      863  control               1010           0\n",
      "6      404  control               1010           0\n",
      "7      346  control               1006           0\n",
      "8      938  control               1005           1\n",
      "9      740  control               1004           0\n",
      "\n",
      "Control group size: 500\n",
      "Variant group size: 500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulate user IDs for the A/B test\n",
    "num_users = 1000\n",
    "user_ids = np.arange(1, num_users + 1)\n",
    "np.random.shuffle(user_ids) # Shuffle to simulate random assignment\n",
    "\n",
    "# Assign users to Control (A) or Variant (B) group\n",
    "# We'll split them roughly 50/50\n",
    "split_point = num_users // 2\n",
    "control_group_users = user_ids[:split_point]\n",
    "variant_group_users = user_ids[split_point:]\n",
    "\n",
    "# Simulate a DataFrame for user interactions in the A/B test\n",
    "# This would typically come from a logging system\n",
    "ab_test_data = pd.DataFrame({\n",
    "    'user_id': user_ids,\n",
    "    'group': ['control'] * len(control_group_users) + ['variant'] * len(variant_group_users),\n",
    "    'product_id_viewed': np.random.choice(df['Product_ID'], num_users), # Simulate users viewing a random product\n",
    "    'conversion': np.random.choice([0, 1], num_users, p=[0.9, 0.1]) # Simulate conversion (e.g., 10% conversion rate overall)\n",
    "})\n",
    "\n",
    "# For simplicity, let's assume conversion rates based on group\n",
    "# Control group conversion rate: 8%\n",
    "# Variant group conversion rate: 12% (simulating a positive uplift due to optimization)\n",
    "ab_test_data['conversion'] = ab_test_data.apply(\n",
    "    lambda row: np.random.choice([0, 1], p=[0.92, 0.08]) if row['group'] == 'control' else np.random.choice([0, 1], p=[0.88, 0.12]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nSimulated A/B Test Data (first 10 rows):\")\n",
    "print(ab_test_data.head(10))\n",
    "\n",
    "print(f\"\\nControl group size: {len(control_group_users)}\")\n",
    "print(f\"Variant group size: {len(variant_group_users)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5ccda",
   "metadata": {},
   "source": [
    "Step 7: Statistical Analysis of A/B Test Results ðŸ”¬\n",
    "After running the experiment for a sufficient period and collecting enough data, we analyze the results to determine if the variant performed significantly better than the control. We'll use statistical hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ca5379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Control Group Conversion Rate: 0.0840\n",
      "Variant Group Conversion Rate: 0.1360\n",
      "Z-statistic: 2.6277\n",
      "P-value: 0.0086\n",
      "\n",
      "Result: The difference is statistically significant. Reject the null hypothesis.\n",
      "Conclusion: The Variant (optimized description) likely led to a higher conversion rate.\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate conversion rates for each group\n",
    "control_conversions = ab_test_data[ab_test_data['group'] == 'control']['conversion'].sum()\n",
    "control_total = len(ab_test_data[ab_test_data['group'] == 'control'])\n",
    "control_cr = control_conversions / control_total if control_total > 0 else 0\n",
    "\n",
    "variant_conversions = ab_test_data[ab_test_data['group'] == 'variant']['conversion'].sum()\n",
    "variant_total = len(ab_test_data[ab_test_data['group'] == 'variant'])\n",
    "variant_cr = variant_conversions / variant_total if variant_total > 0 else 0\n",
    "\n",
    "print(f\"\\nControl Group Conversion Rate: {control_cr:.4f}\")\n",
    "print(f\"Variant Group Conversion Rate: {variant_cr:.4f}\")\n",
    "\n",
    "# Perform a Z-test for two proportions\n",
    "# H0 (Null Hypothesis): There is no difference in conversion rates between groups.\n",
    "# H1 (Alternative Hypothesis): There is a difference in conversion rates between groups.\n",
    "\n",
    "# Number of successes (conversions) and observations (total users) for each group\n",
    "n_control = control_total\n",
    "x_control = control_conversions\n",
    "n_variant = variant_total\n",
    "x_variant = variant_conversions\n",
    "\n",
    "# Calculate pooled proportion\n",
    "p_pooled = (x_control + x_variant) / (n_control + n_variant)\n",
    "\n",
    "# Calculate standard error\n",
    "se = np.sqrt(p_pooled * (1 - p_pooled) * (1/n_control + 1/n_variant))\n",
    "\n",
    "# Calculate Z-statistic\n",
    "if se == 0: # Avoid division by zero if sample sizes are tiny or p_pooled is 0 or 1\n",
    "    z_stat = 0\n",
    "else:\n",
    "    z_stat = (variant_cr - control_cr) / se\n",
    "\n",
    "# Calculate p-value (two-tailed test)\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "\n",
    "print(f\"Z-statistic: {z_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Determine statistical significance\n",
    "alpha = 0.05 # Significance level\n",
    "if p_value < alpha:\n",
    "    print(\"\\nResult: The difference is statistically significant. Reject the null hypothesis.\")\n",
    "    print(\"Conclusion: The Variant (optimized description) likely led to a higher conversion rate.\")\n",
    "else:\n",
    "    print(\"\\nResult: The difference is NOT statistically significant. Fail to reject the null hypothesis.\")\n",
    "    print(\"Conclusion: There is no strong evidence that the Variant improved conversion rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cce531",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 8: Decision Making and Deployment ðŸš€\n",
    "Based on the A/B test analysis:\n",
    "\n",
    "If the variant performs significantly better: Deploy the optimized descriptions to all users.\n",
    "\n",
    "If there's no significant difference or the control performs better: Stick with the original, or iterate on a new variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 9: Monitoring and Iteration ðŸ”„\n",
    "After deployment, continuously monitor the performance of the new descriptions to ensure the uplift is sustained. This also feeds into future optimization cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c9c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
